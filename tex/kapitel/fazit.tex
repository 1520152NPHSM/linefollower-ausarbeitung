\chapter{Fazit} % (fold)
\label{cha:fazit}

Die Implementierung eines Linienfolgers durch Reinforcement Learning bietet im Gegensatz zur Implementierung mit programmatischer Abdeckung aller möglichen Situationen den Vorteil, dass auf vergessene, versteckete oder vorher gänzluch unbekannte Situationen vollkommen autonom bewältigt werden können. Die Erprobung des Verfahrens anhand zweier mittels Lejos implementierten Linienfolgers zeigt, dass das Verfahren nicht immer gegen sinnvolle Strategien konvergiert. Die optimale Strategie fluktuiert bei beiden Robotern nach 2000 Lernschritten noch übermässig. Dies ist zum Teil auf nicht optimale Kombinationen aus Q-Learning-Pa\-ra\-metern und durch die Plattform vorgegeben baulichen Beschränkungen zurückzuführen. Auch ist der Erfolg des gelernten Verhaltens von der Bauweise und der Anzahl der Sensoren und den daraus resultierenden unterschiedlich großen Zustandsräumen abhängig. Zusammenfassend braucht es noch weitere Versuchsreihen um bessere Parameterkombinationen zu finden. Die während der Versuchsreihe ausgeführten Aktionen konvergieren Zeitweise optimal, werden aber meist später von ungünstigen Strategien ersetzt. Für weiter Versuchsreihen wäre es von Interesse die Linienfolger vor einer Kurve etwas Rückwärtsfahren zu lassen.

% chapter fazit (end)